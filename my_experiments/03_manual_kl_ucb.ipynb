{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rs-datasets in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (0.5.1)\n",
      "Requirement already satisfied: datatable in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from rs-datasets) (1.0.0)\n",
      "Requirement already satisfied: pandas in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from rs-datasets) (1.1.5)\n",
      "Requirement already satisfied: gdown in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from rs-datasets) (4.7.1)\n",
      "Requirement already satisfied: pyarrow in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from rs-datasets) (12.0.1)\n",
      "Requirement already satisfied: tqdm in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from rs-datasets) (4.65.0)\n",
      "Requirement already satisfied: xlrd in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from rs-datasets) (2.0.1)\n",
      "Requirement already satisfied: kaggle in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from rs-datasets) (1.5.13)\n",
      "Requirement already satisfied: py7zr in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from rs-datasets) (0.20.5)\n",
      "Requirement already satisfied: openpyxl in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from rs-datasets) (3.1.2)\n",
      "Requirement already satisfied: filelock in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from gdown->rs-datasets) (3.9.0)\n",
      "Requirement already satisfied: requests[socks] in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from gdown->rs-datasets) (2.31.0)\n",
      "Requirement already satisfied: six in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from gdown->rs-datasets) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from gdown->rs-datasets) (4.11.2)\n",
      "Requirement already satisfied: certifi in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from kaggle->rs-datasets) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from kaggle->rs-datasets) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from kaggle->rs-datasets) (8.0.1)\n",
      "Requirement already satisfied: urllib3 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from kaggle->rs-datasets) (2.0.3)\n",
      "Requirement already satisfied: et-xmlfile in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from openpyxl->rs-datasets) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from pandas->rs-datasets) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from pandas->rs-datasets) (1.24.3)\n",
      "Requirement already satisfied: texttable in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from py7zr->rs-datasets) (1.6.7)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from py7zr->rs-datasets) (3.18.0)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from py7zr->rs-datasets) (0.15.7)\n",
      "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from py7zr->rs-datasets) (1.0.0)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from py7zr->rs-datasets) (1.0.1)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from py7zr->rs-datasets) (0.2.3)\n",
      "Requirement already satisfied: brotli>=1.0.9 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from py7zr->rs-datasets) (1.0.9)\n",
      "Requirement already satisfied: inflate64>=0.3.1 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from py7zr->rs-datasets) (0.3.1)\n",
      "Requirement already satisfied: psutil in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from py7zr->rs-datasets) (5.9.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from beautifulsoup4->gdown->rs-datasets) (2.3.2.post1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from python-slugify->kaggle->rs-datasets) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from requests[socks]->gdown->rs-datasets) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from requests[socks]->gdown->rs-datasets) (3.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/arqa/MyRePlay/.venv/lib/python3.8/site-packages (from requests[socks]->gdown->rs-datasets) (1.7.1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "! pip install rs-datasets\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import warnings\n",
    "from optuna.exceptions import ExperimentalWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ExperimentalWarning)\n",
    "\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from pyspark.sql import functions as sf, types as st\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from replay.data_preparator import DataPreparator, Indexer\n",
    "from replay.experiment import Experiment\n",
    "from replay.metrics import Coverage, HitRate, MRR, MAP, NDCG, Surprisal\n",
    "from replay.models import (\n",
    "    ALSWrap, \n",
    "    ADMMSLIM, \n",
    "    ItemKNN,\n",
    "    LightFMWrap, \n",
    "    MultVAE, \n",
    "    NeuroMF, \n",
    "    SLIM, \n",
    "    PopRec, \n",
    "    RandomRec,\n",
    "    UCB,\n",
    "    KL_UCB,\n",
    "    Wilson, \n",
    "    Word2VecRec,\n",
    ")\n",
    "\n",
    "from replay.models.base_rec import HybridRecommender\n",
    "from replay.session_handler import State\n",
    "from replay.splitters import DateSplitter\n",
    "from replay.utils import get_log_info\n",
    "from rs_datasets import MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/19 16:18:52 WARN Utils: Your hostname, UX430 resolves to a loopback address: 127.0.1.1; using 192.168.1.64 instead (on interface wlp2s0)\n",
      "23/06/19 16:18:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/06/19 16:18:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "23/06/19 16:18:53 WARN DependencyUtils: Local jar /home/arqa/MyRePlay/my_experiments/jars/replay_2.12-0.1.jar does not exist, skipping.\n",
      "23/06/19 16:18:53 INFO SparkContext: Running Spark version 3.1.3\n",
      "23/06/19 16:18:53 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/06/19 16:18:53 INFO ResourceUtils: ==============================================================\n",
      "23/06/19 16:18:53 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/06/19 16:18:53 INFO ResourceUtils: ==============================================================\n",
      "23/06/19 16:18:53 INFO SparkContext: Submitted application: pyspark-shell\n",
      "23/06/19 16:18:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/06/19 16:18:53 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/06/19 16:18:53 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/06/19 16:18:54 INFO SecurityManager: Changing view acls to: arqa\n",
      "23/06/19 16:18:54 INFO SecurityManager: Changing modify acls to: arqa\n",
      "23/06/19 16:18:54 INFO SecurityManager: Changing view acls groups to: \n",
      "23/06/19 16:18:54 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/06/19 16:18:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(arqa); groups with view permissions: Set(); users  with modify permissions: Set(arqa); groups with modify permissions: Set()\n",
      "23/06/19 16:18:54 INFO Utils: Successfully started service 'sparkDriver' on port 42143.\n",
      "23/06/19 16:18:54 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/06/19 16:18:54 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/06/19 16:18:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/06/19 16:18:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/06/19 16:18:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/06/19 16:18:54 INFO DiskBlockManager: Created local directory at /home/arqa/tmp/blockmgr-5239dd21-82f8-40fd-aba0-1f94b52c368c\n",
      "23/06/19 16:18:54 INFO MemoryStore: MemoryStore started with capacity 2.8 GiB\n",
      "23/06/19 16:18:54 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/06/19 16:18:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/19 16:18:54 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "23/06/19 16:18:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://localhost:4041\n",
      "23/06/19 16:18:54 ERROR SparkContext: Failed to add jars/replay_2.12-0.1.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /home/arqa/MyRePlay/my_experiments/jars/replay_2.12-0.1.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1929)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1983)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:501)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:501)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/19 16:18:55 INFO Executor: Starting executor ID driver on host 192.168.1.64\n",
      "23/06/19 16:18:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46311.\n",
      "23/06/19 16:18:55 INFO NettyBlockTransferService: Server created on localhost:46311\n",
      "23/06/19 16:18:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/06/19 16:18:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 46311, None)\n",
      "23/06/19 16:18:55 INFO BlockManagerMasterEndpoint: Registering block manager localhost:46311 with 2.8 GiB RAM, BlockManagerId(driver, localhost, 46311, None)\n",
      "23/06/19 16:18:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 46311, None)\n",
      "23/06/19 16:18:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 46311, None)\n",
      "23/06/19 16:18:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/arqa/MyRePlay/my_experiments/spark-warehouse').\n",
      "23/06/19 16:18:55 INFO SharedState: Warehouse path is 'file:/home/arqa/MyRePlay/my_experiments/spark-warehouse'.\n"
     ]
    }
   ],
   "source": [
    "spark = State().session\n",
    "spark\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "logger = logging.getLogger(\"replay\")\n",
    "\n",
    "K = 10\n",
    "K_list_metrics = [1, 5, 10]\n",
    "BUDGET = 20\n",
    "BUDGET_NN = 10\n",
    "SEED = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "users\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>85711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>94043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  gender age  occupation zip_code\n",
       "0        1      24   M  technician    85711\n",
       "1        2      53   F       other    94043\n",
       "2        3      23   M      writer    32067"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "items\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>imdb_url</th>\n",
       "      <th>unknown</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children's</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>...</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?GoldenEye%20(...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Four%20Rooms%...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id              title release_date  \\\n",
       "0        1   Toy Story (1995)  01-Jan-1995   \n",
       "1        2   GoldenEye (1995)  01-Jan-1995   \n",
       "2        3  Four Rooms (1995)  01-Jan-1995   \n",
       "\n",
       "                                            imdb_url  unknown  Action  \\\n",
       "0  http://us.imdb.com/M/title-exact?Toy%20Story%2...    False   False   \n",
       "1  http://us.imdb.com/M/title-exact?GoldenEye%20(...    False    True   \n",
       "2  http://us.imdb.com/M/title-exact?Four%20Rooms%...    False   False   \n",
       "\n",
       "   Adventure  Animation  Children's  Comedy  ...  Fantasy  Film-Noir  Horror  \\\n",
       "0      False       True        True    True  ...    False      False   False   \n",
       "1       True      False       False   False  ...    False      False   False   \n",
       "2      False      False       False   False  ...    False      False   False   \n",
       "\n",
       "   Musical  Mystery  Romance  Sci-Fi  Thriller    War  Western  \n",
       "0    False    False    False   False     False  False    False  \n",
       "1    False    False    False   False      True  False    False  \n",
       "2    False    False    False   False      True  False    False  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = MovieLens(\"100k\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19-Jun-23 16:18:59, replay, INFO: Columns with ids of users or items are present in mapping. The dataframe will be treated as an interactions log.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+-------------------+\n",
      "|user_id|item_id|relevance|          timestamp|\n",
      "+-------+-------+---------+-------------------+\n",
      "|    196|    242|      3.0|1997-12-04 18:55:49|\n",
      "|    186|    302|      3.0|1998-04-04 23:22:22|\n",
      "+-------+-------+---------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparator = DataPreparator()\n",
    "\n",
    "log = preparator.transform(columns_mapping={'user_id': 'user_id',\n",
    "                                      'item_id': 'item_id',\n",
    "                                      'relevance': 'rating',\n",
    "                                      'timestamp': 'timestamp'\n",
    "                                     }, \n",
    "                           data=data.ratings)\n",
    "\n",
    "log.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will consider ratings >= 3 as positive feedback. A positive feedback is treated with relevance = 1\n",
    "only_positives_log = log.filter(sf.col('relevance') >= 3).withColumn('relevance', sf.lit(1))\n",
    "only_positives_log.count()\n",
    "\n",
    "# we will use only algorithms which do not require user and item features and thus set feature dataframes to None\n",
    "user_features=None\n",
    "item_features=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+-------------------+\n",
      "|user_idx|item_idx|relevance|          timestamp|\n",
      "+--------+--------+---------+-------------------+\n",
      "|     645|     287|        1|1997-12-04 18:55:49|\n",
      "|     382|      37|        1|1998-04-04 23:22:22|\n",
      "+--------+--------+---------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = Indexer(user_col='user_id', item_col='item_id')\n",
    "\n",
    "indexer.fit(users=log.select('user_id'),\n",
    "           items=log.select('item_id'))\n",
    "\n",
    "log_replay = indexer.transform(df=only_positives_log)\n",
    "log_replay.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train info:\n",
      " total lines: 66016, total users: 752, total items: 1501\n",
      "test info:\n",
      " total lines: 2279, total users: 104, total items: 824\n",
      "train is cashed: True\n",
      "opt train is cashed:  True\n"
     ]
    }
   ],
   "source": [
    "# train/test split \n",
    "train_spl = DateSplitter(\n",
    "    test_start=0.2,\n",
    "    drop_cold_items=True,\n",
    "    drop_cold_users=True,\n",
    "\n",
    ")\n",
    "train, test = train_spl.split(log_replay)\n",
    "print('train info:\\n', get_log_info(train))\n",
    "print('test info:\\n', get_log_info(test))\n",
    "\n",
    "print(\"train is cashed:\", train.is_cached)\n",
    "\n",
    "# train/test split for hyperparameters selection\n",
    "opt_train, opt_val = train_spl.split(train)\n",
    "opt_train.count(), opt_val.count()\n",
    "\n",
    "print(\"opt train is cashed: \", opt_train.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+-------------------+\n",
      "|user_idx|item_idx|relevance|          timestamp|\n",
      "+--------+--------+---------+-------------------+\n",
      "|     645|     287|        1|1997-12-04 18:55:49|\n",
      "|     283|     124|        1|1998-01-07 17:20:06|\n",
      "+--------+--------+---------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# negative feedback will be used for Wilson and UCB models\n",
    "only_negatives_log = indexer.transform(df=log.filter(sf.col('relevance') < 3).withColumn('relevance', sf.lit(0.)))\n",
    "test_start = test.agg(sf.min('timestamp')).collect()[0][0]\n",
    "\n",
    "# train with both positive and negative feedback\n",
    "pos_neg_train=(train\n",
    "              .withColumn('relevance', sf.lit(1.))\n",
    "              .union(only_negatives_log.filter(sf.col('timestamp') < test_start))\n",
    "             )\n",
    "pos_neg_train.cache()\n",
    "pos_neg_train.count()\n",
    "\n",
    "train.show(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL-UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pos_neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80253\n",
      "+--------+-----+-----+\n",
      "|item_idx|  pos|total|\n",
      "+--------+-----+-----+\n",
      "|      18|276.0|  310|\n",
      "|      14|301.0|  313|\n",
      "|     403| 58.0|   64|\n",
      "+--------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_counts_aggr = log.groupby(\"item_idx\").agg(\n",
    "    sf.sum(\"relevance\").alias(\"pos\"),\n",
    "    sf.count(\"relevance\").alias(\"total\"),\n",
    ")\n",
    "\n",
    "full_count = log.count()\n",
    "print(full_count)\n",
    "items_counts_aggr.show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to/from pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import brentq\n",
    "import numpy as np\n",
    "coef = 0.0\n",
    "\n",
    "def get_ucb(row) :\n",
    "    p = row.pos / row.total\n",
    "    eps = 1e-12\n",
    "    rhs = np.log(full_count) + coef * np.log(np.log(full_count))\n",
    "    \n",
    "    if (p == 0) :\n",
    "        def problem(q) : \n",
    "            return np.log(1/(1-q)) - rhs\n",
    "        ucb = brentq(problem, 0, 1-eps)\n",
    "        return ucb\n",
    "    \n",
    "    if (p == 1) :\n",
    "        def problem(q) : \n",
    "            return np.log(1/q) - rhs\n",
    "        ucb = brentq(problem, 0+eps, 1)\n",
    "        return ucb\n",
    "        \n",
    "    def Bernoulli_KL(p,q) :\n",
    "        return p * np.log(p/q) + (1-p) * np.log((1-p)/(1-q))\n",
    "    \n",
    "    def problem(q) :\n",
    "        return row.total * Bernoulli_KL(p, q) - rhs\n",
    "    \n",
    "    ucb = brentq(problem, p, 1-eps)\n",
    "    return ucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|item_idx|         relevance|\n",
      "+--------+------------------+\n",
      "|      18|0.9557570882379833|\n",
      "|      14| 0.993369593851037|\n",
      "|     403| 0.994167574847216|\n",
      "+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+------------------+\n",
      "|item_idx|         relevance|\n",
      "+--------+------------------+\n",
      "|      18|0.9557570882379833|\n",
      "|      14| 0.993369593851037|\n",
      "|     403| 0.994167574847216|\n",
      "+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+------------------+\n",
      "|item_idx|         relevance|\n",
      "+--------+------------------+\n",
      "|      18|0.9557570882379833|\n",
      "|      14| 0.993369593851037|\n",
      "|     403| 0.994167574847216|\n",
      "+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+------------------+\n",
      "|item_idx|         relevance|\n",
      "+--------+------------------+\n",
      "|      18|0.9557570882379833|\n",
      "|      14| 0.993369593851037|\n",
      "|     403| 0.994167574847216|\n",
      "+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+------------------+\n",
      "|item_idx|         relevance|\n",
      "+--------+------------------+\n",
      "|      18|0.9557570882379833|\n",
      "|      14| 0.993369593851037|\n",
      "|     403| 0.994167574847216|\n",
      "+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+------------------+\n",
      "|item_idx|         relevance|\n",
      "+--------+------------------+\n",
      "|      18|0.9557570882379833|\n",
      "|      14| 0.993369593851037|\n",
      "|     403| 0.994167574847216|\n",
      "+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+------------------+\n",
      "|item_idx|         relevance|\n",
      "+--------+------------------+\n",
      "|      18|0.9557570882379833|\n",
      "|      14| 0.993369593851037|\n",
      "|     403| 0.994167574847216|\n",
      "+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+------------------+\n",
      "|item_idx|         relevance|\n",
      "+--------+------------------+\n",
      "|      18|0.9557570882379833|\n",
      "|      14| 0.993369593851037|\n",
      "|     403| 0.994167574847216|\n",
      "+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "567 ms ± 44.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pd_df = items_counts_aggr.toPandas()\n",
    "pd_df['relevance'] = pd_df[['pos', 'total']].apply(get_ucb, axis=1)\n",
    "sp_df = State().session.createDataFrame(pd_df[['item_idx', 'relevance']])\n",
    "sp_df.show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import brentq\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "coef = 0.0\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def get_ucb(pos, total) :\n",
    "    p = pos / total\n",
    "    eps = 1e-12\n",
    "    rhs = np.log(full_count) + coef * np.log(np.log(full_count))\n",
    "    \n",
    "    if (p == 0) :\n",
    "        def problem(q) : \n",
    "            return np.log(1/(1-q)) - rhs\n",
    "        ucb = brentq(problem, 0, 1-eps)\n",
    "        return ucb\n",
    "    \n",
    "    if (p == 1) :\n",
    "        def problem(q) : \n",
    "            return np.log(1/q) - rhs\n",
    "        ucb = brentq(problem, 0+eps, 1)\n",
    "        return ucb\n",
    "        \n",
    "    def Bernoulli_KL(p,q) :\n",
    "        return p * np.log(p/q) + (1-p) * np.log((1-p)/(1-q))\n",
    "    \n",
    "    def problem(q) :\n",
    "        return total * Bernoulli_KL(p, q) - rhs\n",
    "    \n",
    "    ucb = brentq(problem, p, 1-eps)\n",
    "    return ucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:===================>                                     (8 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|item_idx|         relevance|\n",
      "+--------+------------------+\n",
      "|      18|0.9557570882379833|\n",
      "|      14| 0.993369593851037|\n",
      "|     403| 0.994167574847216|\n",
      "|     365|0.7888825885230141|\n",
      "|      38|0.9205038472286439|\n",
      "|     348|0.9723769262045451|\n",
      "|     225|0.9894810088361161|\n",
      "|      46|0.9991579806365702|\n",
      "|     343|0.9645615995280864|\n",
      "|     406|0.9709802035192856|\n",
      "|     475|0.9999999014167881|\n",
      "|     257|0.9592150887384809|\n",
      "|     687| 0.914985202296716|\n",
      "|     280|0.9060944651219291|\n",
      "|     300| 0.979202371585167|\n",
      "|     619|0.7458957668891314|\n",
      "|     161|0.9847566821177637|\n",
      "|     263|0.8682098609240727|\n",
      "|     443|0.9966422673284837|\n",
      "|     748|0.9998869021701624|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "items_counts = items_counts_aggr.withColumn(\n",
    "            \"relevance\", get_ucb(\"pos\", \"total\")\n",
    "        )\n",
    "\n",
    "item_popularity = items_counts.drop(\"pos\", \"total\")\n",
    "item_popularity.cache().count()\n",
    "item_popularity.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark's Pandas UDF (DOES NOT WORK, idk why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from typing import Iterator, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "coef = 0.0\n",
    "eps = 1e-12\n",
    "rhs = np.log(full_count) + coef * np.log(np.log(full_count))\n",
    "\n",
    "def Bernoulli_KL(p,q) :\n",
    "    return p * np.log(p/q) + (1-p) * np.log((1-p)/(1-q))\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def get_ucb(iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    ucb = 0.0\n",
    "    for pos, total in iterator:\n",
    "        p = pos / total\n",
    "        ucb = brentq(lambda q: np.log(1/(1-q)) - rhs, 0, 1-eps)\n",
    "        #p = pos / total\n",
    "        #if   (p == 0) :\n",
    "        #    ucb = brentq(lambda q: np.log(1/(1-q)) - rhs, 0, 1-eps)\n",
    "        #elif (p == 1) :\n",
    "        #    ucb = brentq(lambda q: np.log(1/q) - rhs    , 0+eps, 1)\n",
    "        #else :\n",
    "        #    ucb = brentq(lambda q: total * Bernoulli_KL(p, q) - rhs, p, 1-eps)\n",
    "        yield ucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/19 16:23:38 ERROR Executor: Exception in task 0.0 in stage 105.0 (TID 860)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 273, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 81, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 266, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 356, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 114, in verify_result_type\n",
      "    raise TypeError(\"Return type of the user-defined function should be \"\n",
      "TypeError: Return type of the user-defined function should be Pandas.Series, but is <class 'float'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/06/19 16:23:38 ERROR TaskSetManager: Task 0 in stage 105.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 273, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 81, in dump_stream\n    for batch in iterator:\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 266, in init_stream_yield_batches\n    for series in iterator:\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 356, in func\n    for result_batch, result_type in result_iter:\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 114, in verify_result_type\n    raise TypeError(\"Return type of the user-defined function should be \"\nTypeError: Return type of the user-defined function should be Pandas.Series, but is <class 'float'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39585/205439108.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mitem_popularity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"total\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mitem_popularity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyRePlay/.venv/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 273, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 81, in dump_stream\n    for batch in iterator:\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 266, in init_stream_yield_batches\n    for series in iterator:\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 356, in func\n    for result_batch, result_type in result_iter:\n  File \"/home/arqa/MyRePlay/.venv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 114, in verify_result_type\n    raise TypeError(\"Return type of the user-defined function should be \"\nTypeError: Return type of the user-defined function should be Pandas.Series, but is <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "items_counts = items_counts_aggr.withColumn(\n",
    "            \"relevance\", get_ucb(\"pos\", \"total\")\n",
    "        )\n",
    "\n",
    "item_popularity = items_counts.drop(\"pos\", \"total\")\n",
    "item_popularity.show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark's Pandas UDF + SciPy Multivariate Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def get_ucb(pos: pd.Series, total: pd.Series) -> pd.Series:\n",
    "    # NOT IMPLEMENTED"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
